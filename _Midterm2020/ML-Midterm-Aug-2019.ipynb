{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Midterm Solution, Aug Semester, 2019\n",
    "\n",
    "In this exam, you will demonstrate your understanding of the material from the lectures, tutorials, and problem sets.\n",
    "\n",
    "For each question, insert your answer directly in this sheet. When complete, export the sheet as a PDF and upload to Gradescope.\n",
    "\n",
    "Note that you have **2.5 hours** to do the exam. Also note that there is at least one short answer question that you may be able to answer faster than the coding questions. You might consider answering those questions first to get as much credit as possible!\n",
    "\n",
    "## Question 1 (20 points)\n",
    "\n",
    "Suppose you enjoy running for exercise, have great data science skills, and would like to build tools to help runners train effectively. You would like to predict a runner's future performance based on his or her past performances, and perhaps come up with some analytics about his or her fitness (increasing, decreasing, etc.)\n",
    "\n",
    "Consider the training and testing data given in the cell below. The data are measurements of Matt's runs from mid June to mid September 2019. The $\\mathtt{X}$ data contain variables for pace, distance, average temperature, and the number of hours of rest since the last run. The $\\mathbf{y}$ data are the average heart rate in beats per minute over each run. These data come from Garmin and OpenWeatherMap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Columns:\n",
    "# 0: average pace in minutes per kilometer\n",
    "# 1: distance in meters\n",
    "# 2: avg apparent temperature (Farenheight, includes humidity factor etc.)\n",
    "# 3: number of hours of rest since the last run\n",
    "\n",
    "Xtrain = np.array(\n",
    "      [[5.6041250e+00, 1.0386240e+04, 9.6480003e+01, 1.6693195e+02],\n",
    "       [4.5901036e+00, 3.3501699e+03, 9.0070000e+01, 1.4042222e+01],\n",
    "       [5.5853438e+00, 1.0589420e+04, 9.6894997e+01, 4.9480556e+01],\n",
    "       [8.4602365e+00, 5.2353198e+03, 9.2915001e+01, 4.8527222e+01],\n",
    "       [5.2759314e+00, 1.0260830e+04, 8.0415001e+01, 6.9376945e+01],\n",
    "       [5.0050054e+00, 1.0308060e+04, 7.8995003e+01, 7.2154442e+01],\n",
    "       [5.1679587e+00, 1.0297140e+04, 8.4269997e+01, 2.4243610e+01],\n",
    "       [5.5909653e+00, 1.6628650e+04, 9.5754997e+01, 5.8910278e+01],\n",
    "       [5.3112388e+00, 1.0397030e+04, 9.1875000e+01, 1.2679722e+01],\n",
    "       [5.0704794e+00, 1.0266610e+04, 8.7489998e+01, 4.8466110e+01],\n",
    "       [5.1297836e+00, 8.6998496e+03, 8.6510002e+01, 4.7598888e+01],\n",
    "       [5.8092251e+00, 2.1145150e+04, 9.6199997e+01, 5.9379166e+01],\n",
    "       [5.1203279e+00, 1.0201160e+04, 9.2875000e+01, 8.4886391e+01],\n",
    "       [5.1424460e+00, 1.0161310e+04, 8.9500000e+01, 2.3730000e+01],\n",
    "       [5.9502554e+00, 2.4152381e+04, 8.7389999e+01, 5.9973331e+01],\n",
    "       [5.4752522e+00, 1.2884080e+04, 9.2269997e+01, 3.6443890e+01],\n",
    "       [6.2727389e+00, 6.8037402e+03, 9.0209999e+01, 2.3653889e+01],\n",
    "       [5.6941118e+00, 1.6106010e+04, 8.7489998e+01, 2.4058611e+01],\n",
    "       [5.6941118e+00, 8.0607300e+03, 9.0735001e+01, 5.0604721e+01],\n",
    "       [5.1313629e+00, 1.4493960e+04, 9.4910004e+01, 2.3852777e+01],\n",
    "       [5.8397570e+00, 6.4594302e+03, 9.5209999e+01, 1.5108278e+02],\n",
    "       [6.0342746e+00, 2.1100061e+04, 9.2345001e+01, 1.9518888e+01],\n",
    "       [5.7790108e+00, 1.4495710e+04, 8.3410004e+01, 6.7106941e+01],\n",
    "       [5.4059896e+00, 1.6109050e+04, 8.6989998e+01, 2.4075834e+01],\n",
    "       [8.1819668e+00, 6.5416201e+03, 8.1650002e+01, 2.3606112e+01],\n",
    "       [5.6230321e+00, 8.0613101e+03, 9.4324997e+01, 2.5450277e+01],\n",
    "       [5.9101658e+00, 2.4156170e+04, 9.1650002e+01, 3.4091110e+01],\n",
    "       [5.1313629e+00, 1.4496380e+04, 8.9084999e+01, 3.6194443e+01],\n",
    "       [5.6650805e+00, 8.0576001e+03, 8.5985001e+01, 2.3728056e+01],\n",
    "       [5.7991180e+00, 1.6350690e+04, 8.9775002e+01, 4.7979168e+01],\n",
    "       [5.8234339e+00, 8.0631299e+03, 9.4845001e+01, 2.6273333e+01],\n",
    "       [5.7254095e+00, 2.5760990e+04, 9.6279999e+01, 3.4454166e+01],\n",
    "       [5.4324207e+00, 1.2948430e+04, 9.0654999e+01, 3.6110554e+01],\n",
    "       [5.7097182e+00, 8.0617100e+03, 8.5294998e+01, 2.3565277e+01],\n",
    "       [5.6727929e+00, 1.2387530e+04, 9.2535004e+01, 2.4852501e+01],\n",
    "       [5.4024849e+00, 8.1778799e+03, 8.8980003e+01, 4.7656387e+01],\n",
    "       [5.5242515e+00, 1.9322131e+04, 7.9985001e+01, 4.6826668e+01],\n",
    "       [5.1234756e+00, 1.6099910e+04, 8.3794998e+01, 2.4853611e+01],\n",
    "       [5.8418040e+00, 6.7139800e+03, 8.3955002e+01, 2.4078611e+01],\n",
    "       [5.5151114e+00, 1.7712240e+04, 8.5815002e+01, 2.3572500e+01]])\n",
    "\n",
    "# Outcome: average heart rate, in beats per minute\n",
    "\n",
    "ytrain = np.array([[150., 146., 148., 152., 147., 152., 151., 144., 149., 146., 144.,\n",
    "       141., 151., 150., 140., 147., 133., 144., 129., 154., 146., 141.,\n",
    "       155., 151., 111., 138., 141., 151., 134., 150., 134., 150., 149.,\n",
    "       134., 140., 137., 141., 150., 129., 143.]])\n",
    "\n",
    "Xtest = np.array([[5.6516337e+00, 1.1277530e+04, 9.4644997e+01, 5.3126389e+01],\n",
    "       [5.9737158e+00, 2.8977260e+04, 9.4230003e+01, 2.0872499e+01],\n",
    "       [5.4094992e+00, 1.1285160e+04, 7.9794998e+01, 4.6364723e+01],\n",
    "       [5.3885121e+00, 1.9334391e+04, 8.2080002e+01, 2.2815277e+01],\n",
    "       [4.9500046e+00, 1.6110120e+04, 8.3410004e+01, 4.8843613e+01],\n",
    "       [5.9269800e+00, 8.0592202e+03, 9.9250000e+01, 2.7060556e+01],\n",
    "       [6.0496063e+00, 3.2190711e+04, 9.3800003e+01, 2.4073334e+01],\n",
    "       [6.6427526e+00, 9.8029502e+03, 8.5150002e+01, 4.4903057e+01],\n",
    "       [5.4112554e+00, 1.0247100e+04, 8.1955002e+01, 2.4426390e+01],\n",
    "       [5.5853438e+00, 2.2543590e+04, 8.4849998e+01, 2.3573610e+01],\n",
    "       [4.0709982e+00, 2.5134600e+03, 9.4324997e+01, 2.7191111e+01],\n",
    "       [5.9038849e+00, 9.6685303e+03, 8.8099998e+01, 2.3758333e+01],\n",
    "       [5.5315852e+00, 1.9787490e+04, 9.7684998e+01, 3.3159168e+01],\n",
    "       [5.1062093e+00, 1.2883740e+04, 7.9375000e+01, 5.9931667e+01],\n",
    "       [6.1591525e+00, 9.6694502e+03, 8.2260002e+01, 2.4936388e+01],\n",
    "       [5.4914880e+00, 1.2887790e+04, 9.7154999e+01, 5.1264168e+01]])\n",
    "\n",
    "ytest = np.array([[134., 144., 145., 142., 153., 130., 139., 122., 141., 140., 129.,\n",
    "       129., 142., 143., 122., 141.]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Q1 continued) **Do the following:**\n",
    "\n",
    "1. **Build a linear regresion model for the training data. Give the optimal parameter vector $\\mathbf{\\theta}$ below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Plot predicted heart rate against actual heart rate for the training set and the test set using these parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (20 points)\n",
    "\n",
    "As it turns out, there is one outlier in the training data, when Matt's heart rate monitor was not fixed correctly against the skin, resulting in a strange average heart rate.\n",
    "\n",
    "**Do the following:**\n",
    "\n",
    "1. **Plot pace (column 0) against heart rate ($\\textbf{y}$) for the training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Give the outlier point's data here and explain why it does not seem to be consistent with the other data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier point is the one in the upper right corner of the previous plot. The pace is very slow, so the heart rate should be correspondingly low (like the other point in the lower right corner of the plot) but it is not. The actual data point is the fourth row of Xtest and fourth element of ytest:\n",
    "\n",
    "$$\\textbf{x} = \\begin{bmatrix} 8.46 & 5235 & 92.9 & 48.5 \\end{bmatrix}^T, \\; \\; y = 152$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Build a new regression model without this point, and make a new plot of predicted versus actual heart rate for the training set and test set using the new parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Briefly discuss whether the result based on the \"cleaned\" dataset is better than the result on the original data set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model based on the cleaned dataset is much better.\n",
    "\n",
    "The original parameter estimate had a positive value for $\\theta_1$, which says that as minutes per kilometer gets bigger (pace is slower), heart rate increases, which is counterintuitive.\n",
    "\n",
    "The new parameter estimate has a negative value for the variable, as we would expect. The value of -11.9 says that if we speed up by one minute per kilometer, heart rate will increase by 12 beats per minute, which seems reasonable.\n",
    "\n",
    "The RMSE on the training set decreased substantially, from 7.0 to 4.6. The test set RMSE also improved albeit not as much, from 9.4 to 8.9. We see that on the test set, the predictions seem to be higher than the actual rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (20 points)\n",
    "\n",
    "Actually, the data in Questions 1 and 2 are not quite independent of each other, as they form a sequence. Performance in a sport improves with training over time.\n",
    "\n",
    "Perhaps we can model the effect of training on performance by adding a new variable to the data set that is the number of days since the training began.\n",
    "\n",
    "Create a new variable indicating the number of days that have passed since training began. Let's name this variable $x_5$, and let $x_4$ be the \"hours of rest since the last run\" variable (last column in the $\\texttt{X}$ data set). Let $x_5^{(0)} = 0$, and let\n",
    "\n",
    "$$x_5^{(i)} = \\frac{1}{24}\\sum_{j=1}^{i-1} x_4^{(i)}.$$ \n",
    "\n",
    "You can calculate this more easily using the recurrence\n",
    "\n",
    "$$x_5^{(i)} = x_5^{(i-1)} + \\frac{1}{24}x_4^{(i)}.$$ \n",
    "\n",
    "**Do the following:**\n",
    "\n",
    "1. **Add the new variable $x_5$ to your test and training datasets. Note that the test data are just a continuation of the series from the training data, so $x_5$ for the first entry in the test set should be $x_5$ for the last entry in the training set plus the number of rest hours for the first entry in the test set divided by 24.**\n",
    "\n",
    "2. **Build a new regression model using the augmented data set, give the optimal parameters $\\mathbf{\\theta}$ here, and show the scatter plot of predicted versus actual outcomes for the training set and test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new parameter estimate has a negative load on number of training days, indicating that, as we might expect, when the number of training days increases, the heart rate is decreasing. The value of -0.05 says that every 20 days of training reduces the heart rate by 1 beat per minute. That seems like a small effect, but interestingly, though training RMSE only improves slightly with this variable, from 4.56 to 4.48, test RMSE improves quite a bit, from 8.94 to 7.40. The test set has bigger values for the \"days of training\" variable, so it has more effect on the test set.\n",
    "\n",
    "Lastly, we see that the model is still overpredicting heart rate on the test set, indicating that there is still something missing in our predictors. We might consider adding a \"volume of training\" variable that reflects the amount of training in the past week or so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (20 points)\n",
    "\n",
    "Next we consider a classification problem. We know that logistic regression is only capable of modeling linear classification boundaries. In this question, you will synthesize a data set that has a nonlinear classification boundary then show that we can obtain a good logistic regression classifier by transforming the data so that an appropriate classification boundary is linear in the new transformed inputs.\n",
    "\n",
    "**Do the following:**\n",
    "\n",
    "1. **Generate 100 data points for class 1 in which $x_1$ is sampled uniformly from the range [-3..3] and $x_2$ is sampled from a Gaussian with mean $x_1^2 + 2$ and standard deviation 1.0.**\n",
    "\n",
    "2. **Generate 100 data points for class 2 in which $x_1$ is sampled uniformly from the range [-3..3] and $x_2$ is sampled from a Gaussian with mean $x_1^2 + 5$ and standard deviation 1.0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Split the dataset into 80% train and 20% test, estimate a logistic regression model, and report the model's training set accuracy, test set accuracy, and optimal parameters here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Transform the data to contain five input variables: $x_1$, $x_2$, $x_1^2$, $x_2^2$, and $x_1x_2$. Build a new logistic regression model, report its training and test set accuracy, and optimal parameters here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (20 points)\n",
    "\n",
    "Suppose you were interested in predicting the number of traffic fatalities that occur at a given traffic intersection based on various factors that might indicate the level of danger at that intersection: number of lanes, speed limit, types and number of businesses close to the intersection, curvature of the roads leading to the intersection, number of people that live in the surrounding area, and so on.\n",
    "\n",
    "Imagine that you obtain monthly fatality counts for 500 intersections in Bangkok and also obtain the necessary input data on the characteristics of the intersections then start to model.\n",
    "\n",
    "The standard linear regression model would consider the outcome (the number of traffic fatalities per month) as a Gaussian random variable with a mean of $\\mathbf{\\theta}^T\\mathbf{x}$. But that might not be the best approach for this problem.\n",
    "\n",
    "For example, you might consider the [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution).\n",
    "\n",
    "**Do the following.**\n",
    "\n",
    "1. **Read briefly about the Poisson distribution and explain here why it would be a better model of the outcome (number of traffic fatalities per month at a particular intersection) than a Gaussian.**\n",
    "\n",
    "2. **Noting that the Poisson distribution is a member of the exponential family, explain in general how you could use the GLM approach to come up with a model that predicts the fatality rate to be a Poisson random variable with mean** $\\lambda = \\exp(\\mathbf{\\theta}^T\\textbf{x})$. **(Note that the exponential here is just to convert the linear combination of inputs, which could be positive or negative, to a strictly positive value.)**\n",
    "\n",
    "Note that for part 2, *you do not need to derive any equations*! Just explain what steps you would take to obtain a good machine learning algorithm for this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Solution:**\n",
    "\n",
    "1. A Gaussian random variable has a range of $-\\infty$ to $\\infty$, with nonzero probability for every value in that range. Clearly, the probability of observing a negative number of fatalities at an intersection is zero. The Gaussian would have to be modified somehow (truncation?) to accurately model the observation. The truncated Gaussian probably wouldn't be a member of the exponential family any more.\n",
    "\n",
    "   A Poisson random variable, on the other hand, models exactly what we want, i.e., a distribution over the number $k$ of events occurring in a given range of time. It is only valid for integer values of $k \\ge 0$. \n",
    "\n",
    "2. We would first model the Poisson as a member of the exponential family, with parameter $\\lambda$, the expected number of fatalities per month at a given intersection. This will allow us to write down the log likelihood function, take the gradient of the log likelihood with respect to $\\mathbf{\\theta}$, and thereby arrive at an estimation algorithm for $\\mathbf{\\theta}$. Once we have the estimation algorithm, given a dataset, we guess an initial value for $\\mathbf{\\theta}$ and use gradient ascent to find the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "54d292b6f3ca4ff13f504c55e6e4b729c6c0a14070d37d9d8c8aca786423add6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
